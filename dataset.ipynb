{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-29 11:28:18.512846: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import datetime as dt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "\n",
    "from camels_aus.repository import CamelsAus\n",
    "from model.tf.hydro import ProductionStorage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "WINDOW_SIZE = 7\n",
    "\n",
    "camels_dir = '../data/camels/aus'\n",
    "repo = CamelsAus()\n",
    "repo.load_from_text_files(camels_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CamelsDataset(object):\n",
    "    ts_vars = ['precipitation_AWAP', 'et_morton_actual_SILO',\n",
    "                       'tmax_awap', 'tmin_awap']\n",
    "    location_vars = ['state_outlet', 'map_zone', 'catchment_area']\n",
    "    streamflow_vars = ['q_mean', 'stream_elas', 'runoff_ratio', 'high_q_freq',\n",
    "                       'high_q_dur', 'low_q_freq', 'zero_q_freq']\n",
    "    target_vars = ['streamflow_mmd']\n",
    "    ts_slice = slice(dt.datetime(1980, 1, 1), dt.datetime(2015, 1, 1))\n",
    "\n",
    "    def __init__(self, data_dir, state_outlet=None, map_zone=None,\n",
    "                 station_list=None, ts_vars=None, location_vars=None,\n",
    "                 streamflow_vars=None, target_vars=None) -> None:\n",
    "        super().__init__()\n",
    "        self.data_dir = data_dir\n",
    "        compute_flow_cdf = False\n",
    "\n",
    "        # Reassign columns\n",
    "        if ts_vars is not None:\n",
    "            self.ts_vars = ts_vars\n",
    "        if location_vars is not None:\n",
    "            self.location_vars = location_vars\n",
    "        if streamflow_vars is not None:\n",
    "            self.streamflow_vars = streamflow_vars\n",
    "        if target_vars is not None:\n",
    "            self.target_vars = target_vars\n",
    "        \n",
    "        if 'flow_cdf' in self.target_vars:\n",
    "            self.target_vars.remove('flow_cdf')\n",
    "            if 'streamflow_MLd_infilled' not in self.target_vars:\n",
    "                self.target_vars.append('streamflow_MLd_infilled')\n",
    "            compute_flow_cdf = True\n",
    "\n",
    "        # Load the data\n",
    "        self.repo = CamelsAus()\n",
    "        self.repo.load_from_text_files(data_dir)\n",
    "\n",
    "        # # Timeseries data\n",
    "        ts_data = self.repo.daily_data.sel(time=self.ts_slice)[self.ts_vars+self.target_vars].to_dataframe().reset_index()\n",
    "        self.ts_data = ts_data[self.ts_vars + ['station_id', 'time']]\n",
    "        self.targets = ts_data[self.target_vars + ['station_id', 'time']]\n",
    "        \n",
    "        # # Static data\n",
    "        location_data = self.repo.location_attributes.to_dataframe()[self.location_vars]\n",
    "        streamflow_data = self.repo.streamflow_attributes.to_dataframe()[self.streamflow_vars]\n",
    "        self.static_data = pd.concat([location_data, streamflow_data], axis=1).reset_index()\n",
    "\n",
    "        # Filter static data by station_list or state_outlet and map_zone\n",
    "        if station_list is not None:\n",
    "            self.station_list = station_list\n",
    "            self.static_data = self.static_data[self.static_data['station_id'].isin(self.station_list)]\n",
    "            self.ts_data = self.ts_data[self.ts_data['station_id'].isin(self.station_list)]\n",
    "        elif state_outlet is not None and map_zone is not None:\n",
    "            self.static_data = self.static_data[(self.static_data['state_outlet']==state_outlet) & (self.static_data['map_zone']==map_zone)]\n",
    "            self.station_list = self.static_data.station_id.unique()\n",
    "            self.ts_data = self.ts_data[self.ts_data['station_id'].isin(self.station_list)]\n",
    "        else:\n",
    "            raise ValueError('station_list or state_outlet and map_zone must be provided')\n",
    "        self.static_data.drop(columns=self.location_vars, inplace=True)\n",
    "\n",
    "        # Timestamps\n",
    "        self._ts = self.ts_data.time.unique()\n",
    "\n",
    "        # Sort data\n",
    "        self.ts_data = self.ts_data.sort_values(['time', 'station_id'])\n",
    "        self.static_data = self.static_data.sort_values('station_id')\n",
    "        self.targets = self.targets.sort_values(['time', 'station_id'])\n",
    "\n",
    "        if compute_flow_cdf:\n",
    "            self.targets = self.add_flow_cdf(self.targets)\n",
    "            self.targets.drop(columns=['streamflow_MLd_infilled'], inplace=True)\n",
    "    \n",
    "    def add_flow_cdf(self, target_data):\n",
    "        target_data_updated = []\n",
    "        for station_id in self.station_list:\n",
    "            df = target_data[target_data.station_id == station_id].set_index('time')\n",
    "            if 'streamflow_MLd_infilled' in self.target_vars:\n",
    "                flow_values = df.streamflow_MLd_infilled.dropna().sort_values(ascending=True)\n",
    "            else:\n",
    "                flow_values = df.streamflow_mmd.dropna().sort_values(ascending=True)\n",
    "            flow_cdf = (np.arange(len(flow_values))+1)/(len(flow_values) + 1)\n",
    "            flow_cdf = pd.Series(flow_cdf, index=flow_values.index)\n",
    "            df['flow_cdf'] = flow_cdf\n",
    "            df.reset_index(inplace=True)\n",
    "            target_data_updated.append(df)\n",
    "        return pd.concat(target_data_updated)\n",
    "        \n",
    "\n",
    "    def create_datasets(self, ts_data, static_data, target_data):\n",
    "        station_ids = static_data.station_id.unique()\n",
    "        \n",
    "        ts_arr = []\n",
    "        static_arr = []\n",
    "        target_arr = []\n",
    "        station_names = []\n",
    "        \n",
    "        for station_id in station_ids:\n",
    "            \n",
    "            station_ts = ts_data[ts_data['station_id']==station_id].drop(columns=['station_id', 'time'])\n",
    "            station_static = static_data[static_data['station_id']==station_id].drop(columns=['station_id'])\n",
    "            station_targets = target_data[target_data['station_id']==station_id].drop(columns=['station_id', 'time'])\n",
    "            \n",
    "            station_ts_data, station_targets = self.create_sequences(station_ts.values, station_targets.values, WINDOW_SIZE)\n",
    "            station_static_data = np.repeat(station_static.values, station_ts_data.shape[0], axis=0)\n",
    "            station_names_data = np.repeat(station_id, station_ts_data.shape[0], axis=0)[:, np.newaxis]\n",
    "            \n",
    "            ts_arr.append(station_ts_data)\n",
    "            static_arr.append(station_static_data)\n",
    "            target_arr.append(station_targets)\n",
    "            station_names.append(station_names_data)\n",
    "        \n",
    "        self.ts_arr = np.concatenate(ts_arr)\n",
    "        self.static_arr = np.concatenate(static_arr)\n",
    "        self.target_arr = np.concatenate(target_arr)\n",
    "        self.station_names = np.concatenate(station_names)\n",
    "\n",
    "    def create_sequences(self, x, y, window_size):\n",
    "        sequences = []\n",
    "        targets = []\n",
    "        for i in range(window_size, len(x)):\n",
    "            sequences.append(x[i-window_size:i])\n",
    "            targets.append(y[i])\n",
    "        return np.stack(sequences), np.stack(targets)\n",
    "    \n",
    "    def get_datasets(self, test_size=0.2, batch_size=32):\n",
    "        # Create the datasets\n",
    "        self.create_datasets(self.ts_data, self.static_data, self.targets)\n",
    "\n",
    "        # Split the data\n",
    "        n_stations = np.unique(self.station_names).shape[0]\n",
    "        n_records = self.station_names.shape[0]/n_stations\n",
    "        n_records_train = int(n_records*(1-test_size)) * n_stations\n",
    "\n",
    "        ts_train = self.ts_arr[:n_records_train]\n",
    "        ts_test = self.ts_arr[n_records_train:]\n",
    "        static_train = self.static_arr[:n_records_train]\n",
    "        static_test = self.static_arr[n_records_train:]\n",
    "        target_train = self.target_arr[:n_records_train]\n",
    "        target_test = self.target_arr[n_records_train:]\n",
    "        station_names_train = self.station_names[:n_records_train]\n",
    "        station_names_test = self.station_names[n_records_train:]\n",
    "\n",
    "        # Conver train and test data to tensors\n",
    "        ts_train = tf.convert_to_tensor(ts_train, dtype=tf.float32, name='timeseries_train')\n",
    "        ts_test = tf.convert_to_tensor(ts_test, dtype=tf.float32, name='timeseries_test')\n",
    "        static_train = tf.convert_to_tensor(static_train, dtype=tf.float32, name='static_train')\n",
    "        static_test = tf.convert_to_tensor(static_test, dtype=tf.float32, name='static_test')\n",
    "        target_train = tf.convert_to_tensor(target_train, dtype=tf.float32, name='target_train')\n",
    "        target_test = tf.convert_to_tensor(target_test, dtype=tf.float32, name='target_test')\n",
    "        station_names_train = tf.convert_to_tensor(station_names_train, dtype=tf.string, name='station_names_train')\n",
    "        station_names_test = tf.convert_to_tensor(station_names_test, dtype=tf.string, name='station_names_test')\n",
    "\n",
    "        # Create the datasets\n",
    "        train_dataset = tf.data.Dataset.from_tensor_slices({'station_id': station_names_train,\n",
    "                                                            'timeseries': ts_train,\n",
    "                                                            'static': static_train,\n",
    "                                                            'target': target_train})\n",
    "        train_dataset = train_dataset.shuffle(140000).batch(batch_size)\n",
    "\n",
    "        test_dataset = tf.data.Dataset.from_tensor_slices({'station_id': station_names_test,\n",
    "                                                           'timeseries': ts_test,\n",
    "                                                           'static': static_test,\n",
    "                                                           'target': target_test})\n",
    "        test_dataset = test_dataset.batch(batch_size)\n",
    "        \n",
    "        return train_dataset, test_dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "camels_ds = CamelsDataset(data_dir=camels_dir, \n",
    "                          station_list=['314213'],\n",
    "                          # state_outlet='WA', map_zone=50\n",
    "                          target_vars=['flow_cdf'])\n",
    "train_ds, test_ds = camels_ds.get_datasets(batch_size=256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(256, 1) (256, 7, 4) (256, 7) (256, 1)\n"
     ]
    }
   ],
   "source": [
    "for batch in train_ds:\n",
    "    print(batch['station_id'].shape, batch['timeseries'].shape, batch['static'].shape, batch['target'].shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HybridDataset(CamelsDataset):\n",
    "\n",
    "    ts_vars = ['precipitation_AWAP', 'et_morton_actual_SILO',\n",
    "                       'tmax_awap', 'tmin_awap']\n",
    "    location_vars = ['state_outlet', 'map_zone']\n",
    "    streamflow_vars = ['q_mean', 'stream_elas', 'runoff_ratio', 'high_q_freq',\n",
    "                       'high_q_dur', 'low_q_freq', 'zero_q_freq']\n",
    "    target_vars = ['streamflow_mmd']\n",
    "    ts_slice = slice(dt.datetime(1980, 1, 1), dt.datetime(2015, 1, 1))\n",
    "\n",
    "    def __init__(self, data_dir, gr4j_logfile, state_outlet=None, map_zone=None, station_list=None) -> None:\n",
    "        super().__init__(data_dir, state_outlet=state_outlet,\n",
    "                         map_zone=map_zone, station_list=station_list)\n",
    "        self.gr4j_logs = pd.read_csv(gr4j_logfile)\n",
    "    \n",
    "    def create_datasets(self, ts_data, static_data, target_data):\n",
    "        station_ids = static_data.station_id.unique()\n",
    "        \n",
    "        ts_arr = []\n",
    "        static_arr = []\n",
    "        target_arr = []\n",
    "        station_names = []\n",
    "        \n",
    "        for station_id in station_ids:\n",
    "            \n",
    "            station_ts = ts_data[ts_data['station_id']==station_id].drop(columns=['station_id', 'time']).values\n",
    "            station_static = static_data[static_data['station_id']==station_id].drop(columns=['station_id']).values\n",
    "            station_targets = target_data[target_data['station_id']==station_id].drop(columns=['station_id', 'time']).values\n",
    "\n",
    "            # Initialize GR4J Production storage\n",
    "            x1_param = self.gr4j_logs.loc[self.gr4j_logs['station_id']==station_id, 'x1'].values[0]\n",
    "            prod = ProductionStorage(x1_param)\n",
    "            station_hybrid_feat = prod(tf.convert_to_tensor(station_ts), include_x=False, scale=False)[0].numpy()\n",
    "            station_ts = np.concatenate([station_ts, station_hybrid_feat], axis=1)\n",
    "            \n",
    "            station_ts_data, station_targets = self.create_sequences(station_ts, station_targets, WINDOW_SIZE)\n",
    "            station_static_data = np.repeat(station_static, station_ts_data.shape[0], axis=0)\n",
    "            station_names_data = np.repeat([station_id], station_ts_data.shape[0], axis=0)[:, np.newaxis]\n",
    "            \n",
    "            ts_arr.append(station_ts_data)\n",
    "            static_arr.append(station_static_data)\n",
    "            target_arr.append(station_targets)\n",
    "            station_names.append(station_names_data)\n",
    "        \n",
    "        self.ts_arr = np.concatenate(ts_arr)\n",
    "        self.static_arr = np.concatenate(static_arr)\n",
    "        self.target_arr = np.concatenate(target_arr)\n",
    "        self.station_names = np.concatenate(station_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "gr4j_logfile = '/Users/akap5486/Library/CloudStorage/OneDrive-UNSW/Shared/Projects/01_PhD/04_DeepGR4J_QNN/deepgr4j-extremes/results/gr4j/result.csv'\n",
    "hybrid_ds = HybridDataset(data_dir=camels_dir, gr4j_logfile=gr4j_logfile, \n",
    "                        #   station_list=['314213'],\n",
    "                          state_outlet='WA', map_zone=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds, test_ds = hybrid_ds.get_datasets(batch_size=256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(256, 1) (256, 7, 8) (256, 7) (256, 1)\n"
     ]
    }
   ],
   "source": [
    "for batch in train_ds:\n",
    "    print(batch['station_id'].shape, batch['timeseries'].shape, batch['static'].shape, batch['target'].shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hydroml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
