{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-28 10:49:02.275919: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import datetime as dt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "\n",
    "from camels_aus.repository import CamelsAus\n",
    "from model.tf.hydro import ProductionStorage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "WINDOW_SIZE = 7\n",
    "\n",
    "camels_dir = '../data/camels/aus'\n",
    "repo = CamelsAus()\n",
    "repo.load_from_text_files(camels_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CamelsDataset(object):\n",
    "    ts_vars = ['precipitation_AWAP', 'et_morton_actual_SILO',\n",
    "                       'tmax_awap', 'tmin_awap', 'streamflow_mmd']\n",
    "    location_vars = ['state_outlet', 'map_zone']\n",
    "    streamflow_vars = ['q_mean', 'stream_elas', 'runoff_ratio', 'high_q_freq',\n",
    "                       'high_q_dur', 'low_q_freq', 'zero_q_freq']\n",
    "    target_vars = ['streamflow_mmd']\n",
    "    ts_slice = slice(dt.datetime(1980, 1, 1), dt.datetime(2015, 1, 1))\n",
    "\n",
    "    def __init__(self, data_dir, state_outlet=None, map_zone=None,\n",
    "                 station_list=None, ts_vars=None, location_vars=None,\n",
    "                 streamflow_vars=None, target_vars=None) -> None:\n",
    "        super().__init__()\n",
    "        self.data_dir = data_dir\n",
    "\n",
    "        # Reassign columns\n",
    "        if ts_vars is not None:\n",
    "            self.ts_vars = ts_vars\n",
    "        if location_vars is not None:\n",
    "            self.location_vars = location_vars\n",
    "        if streamflow_vars is not None:\n",
    "            self.streamflow_vars = streamflow_vars\n",
    "        if target_vars is not None:\n",
    "            self.target_vars = target_vars\n",
    "\n",
    "        # Load the data\n",
    "        self.repo = CamelsAus()\n",
    "        self.repo.load_from_text_files(data_dir)\n",
    "\n",
    "        # # Timeseries data\n",
    "        ts_data = self.repo.daily_data.sel(time=self.ts_slice)[self.ts_vars+self.target_vars].to_dataframe().reset_index()\n",
    "        self.ts_data = ts_data[self.ts_vars + ['station_id', 'time']]\n",
    "        self.targets = ts_data[self.target_vars + ['station_id', 'time']]\n",
    "        \n",
    "        # # Static data\n",
    "        location_data = self.repo.location_attributes.to_dataframe()[self.location_vars]\n",
    "        streamflow_data = self.repo.streamflow_attributes.to_dataframe()[self.streamflow_vars]\n",
    "        self.static_data = pd.concat([location_data, streamflow_data], axis=1).reset_index()\n",
    "\n",
    "        # Filter static data by station_list or state_outlet and map_zone\n",
    "        if station_list is not None:\n",
    "            self.static_data = self.static_data[self.static_data['station_id'].isin(station_list)]\n",
    "            self.ts_data = self.ts_data[self.ts_data['station_id'].isin(station_list)]\n",
    "        elif state_outlet is not None and map_zone is not None:\n",
    "            self.static_data = self.static_data[(self.static_data['state_outlet']==state_outlet) & (self.static_data['map_zone']==map_zone)]\n",
    "            station_list = self.static_data.station_id.unique()\n",
    "            self.ts_data = self.ts_data[self.ts_data['station_id'].isin(station_list)]\n",
    "        else:\n",
    "            raise ValueError('station_list or state_outlet and map_zone must be provided')\n",
    "        self.static_data.drop(columns=['state_outlet', 'map_zone'], inplace=True)\n",
    "\n",
    "        # Timestamps\n",
    "        self._ts = self.ts_data.time.unique()\n",
    "\n",
    "        # Sort data\n",
    "        self.ts_data = self.ts_data.sort_values(['time', 'station_id'])\n",
    "        self.static_data = self.static_data.sort_values('station_id')\n",
    "        self.targets = self.targets.sort_values(['time', 'station_id'])\n",
    "\n",
    "    def create_datasets(self, ts_data, static_data, target_data):\n",
    "        station_ids = static_data.station_id.unique()\n",
    "        \n",
    "        ts_arr = []\n",
    "        static_arr = []\n",
    "        target_arr = []\n",
    "        station_names = []\n",
    "        \n",
    "        for station_id in station_ids:\n",
    "            \n",
    "            station_ts = ts_data[ts_data['station_id']==station_id].drop(columns=['station_id', 'time'])\n",
    "            station_static = static_data[static_data['station_id']==station_id].drop(columns=['station_id'])\n",
    "            station_targets = target_data[target_data['station_id']==station_id].drop(columns=['station_id', 'time'])\n",
    "            \n",
    "            station_ts_data, station_targets = self.create_sequences(station_ts.values, station_targets.values, WINDOW_SIZE)\n",
    "            station_static_data = np.repeat(station_static.values, station_ts_data.shape[0], axis=0)\n",
    "            station_names_data = np.repeat([station_id], station_ts_data.shape[0], axis=0)\n",
    "            \n",
    "            ts_arr.append(station_ts_data)\n",
    "            static_arr.append(station_static_data)\n",
    "            target_arr.append(station_targets)\n",
    "            station_names.append(station_names_data)\n",
    "        \n",
    "        self.ts_arr = np.concatenate(ts_arr)\n",
    "        self.static_arr = np.concatenate(static_arr)\n",
    "        self.target_arr = np.concatenate(target_arr)\n",
    "        self.station_names = np.concatenate(station_names)\n",
    "\n",
    "    def create_sequences(self, x, y, window_size):\n",
    "        sequences = []\n",
    "        targets = []\n",
    "        for i in range(window_size, len(x)):\n",
    "            sequences.append(x[i-window_size:i])\n",
    "            targets.append(y[i])\n",
    "        return np.stack(sequences), np.stack(targets)\n",
    "    \n",
    "    def get_datasets(self, test_size=0.2, batch_size=32):\n",
    "        # Create the datasets\n",
    "        self.create_datasets(self.ts_data, self.static_data, self.targets)\n",
    "\n",
    "        # Split the data\n",
    "        n_stations = np.unique(self.station_names).shape[0]\n",
    "        n_records = self.station_names.shape[0]/n_stations\n",
    "        n_records_train = int(n_records*(1-test_size)) * n_stations\n",
    "\n",
    "        ts_train = self.ts_arr[:n_records_train]\n",
    "        ts_test = self.ts_arr[n_records_train:]\n",
    "        static_train = self.static_arr[:n_records_train]\n",
    "        static_test = self.static_arr[n_records_train:]\n",
    "        target_train = self.target_arr[:n_records_train]\n",
    "        target_test = self.target_arr[n_records_train:]\n",
    "        station_names_train = self.station_names[:n_records_train]\n",
    "        station_names_test = self.station_names[n_records_train:]\n",
    "\n",
    "        # Conver train and test data to tensors\n",
    "        ts_train = tf.convert_to_tensor(ts_train, dtype=tf.float32)\n",
    "        ts_test = tf.convert_to_tensor(ts_test, dtype=tf.float32)\n",
    "        static_train = tf.convert_to_tensor(static_train, dtype=tf.float32)\n",
    "        static_test = tf.convert_to_tensor(static_test, dtype=tf.float32)\n",
    "        target_train = tf.convert_to_tensor(target_train, dtype=tf.float32)\n",
    "        target_test = tf.convert_to_tensor(target_test, dtype=tf.float32)\n",
    "        station_names_train = tf.convert_to_tensor(station_names_train, dtype=tf.string)\n",
    "        station_names_test = tf.convert_to_tensor(station_names_test, dtype=tf.string)\n",
    "\n",
    "        # Create the datasets\n",
    "        train_dataset = tf.data.Dataset.from_tensor_slices((station_names_train, ts_train, \n",
    "                                                            static_train, target_train))\n",
    "        train_dataset = train_dataset.shuffle(140000).batch(batch_size)\n",
    "\n",
    "        test_dataset = tf.data.Dataset.from_tensor_slices((station_names_test, ts_test,\n",
    "                                                           static_test, target_test))\n",
    "        test_dataset = test_dataset.batch(batch_size)\n",
    "        \n",
    "        return train_dataset, test_dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "camels_ds = CamelsDataset(data_dir=camels_dir, \n",
    "                        #   station_list=['314213'],\n",
    "                          state_outlet='WA', map_zone=50)\n",
    "train_ds, test_ds = camels_ds.get_datasets(batch_size=256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HybridDataset(CamelsDataset):\n",
    "\n",
    "    ts_vars = ['precipitation_AWAP', 'et_morton_actual_SILO',\n",
    "                       'tmax_awap', 'tmin_awap']\n",
    "    location_vars = ['state_outlet', 'map_zone']\n",
    "    streamflow_vars = ['q_mean', 'stream_elas', 'runoff_ratio', 'high_q_freq',\n",
    "                       'high_q_dur', 'low_q_freq', 'zero_q_freq']\n",
    "    target_vars = ['streamflow_mmd']\n",
    "    ts_slice = slice(dt.datetime(1980, 1, 1), dt.datetime(2015, 1, 1))\n",
    "\n",
    "    def __init__(self, data_dir, gr4j_logfile, state_outlet=None, map_zone=None, station_list=None) -> None:\n",
    "        super().__init__(data_dir, state_outlet=state_outlet,\n",
    "                         map_zone=map_zone, station_list=station_list)\n",
    "        self.gr4j_logs = pd.read_csv(gr4j_logfile)\n",
    "    \n",
    "    def create_datasets(self, ts_data, static_data, target_data):\n",
    "        station_ids = static_data.station_id.unique()\n",
    "        \n",
    "        ts_arr = []\n",
    "        static_arr = []\n",
    "        target_arr = []\n",
    "        station_names = []\n",
    "        \n",
    "        for station_id in station_ids:\n",
    "            \n",
    "            station_ts = ts_data[ts_data['station_id']==station_id].drop(columns=['station_id', 'time']).values\n",
    "            station_static = static_data[static_data['station_id']==station_id].drop(columns=['station_id']).values\n",
    "            station_targets = target_data[target_data['station_id']==station_id].drop(columns=['station_id', 'time']).values\n",
    "\n",
    "            # Initialize GR4J Production storage\n",
    "            x1_param = self.gr4j_logs.loc[self.gr4j_logs['station_id']==station_id, 'x1'].values[0]\n",
    "            prod = ProductionStorage(x1_param)\n",
    "            station_hybrid_feat = prod(tf.convert_to_tensor(station_ts), include_x=False, scale=False)[0].numpy()\n",
    "            station_ts = np.concatenate([station_ts, station_hybrid_feat], axis=1)\n",
    "            \n",
    "            station_ts_data, station_targets = self.create_sequences(station_ts, station_targets, WINDOW_SIZE)\n",
    "            station_static_data = np.repeat(station_static, station_ts_data.shape[0], axis=0)\n",
    "            station_names_data = np.repeat(station_id, station_ts_data.shape[0], axis=0)\n",
    "            \n",
    "            ts_arr.append(station_ts_data)\n",
    "            static_arr.append(station_static_data)\n",
    "            target_arr.append(station_targets)\n",
    "            station_names.append(station_names_data)\n",
    "        \n",
    "        self.ts_arr = np.concatenate(ts_arr)\n",
    "        self.static_arr = np.concatenate(static_arr)\n",
    "        self.target_arr = np.concatenate(target_arr)\n",
    "        self.station_names = np.concatenate(station_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "gr4j_logfile = '/Users/akap5486/Library/CloudStorage/OneDrive-UNSW/Shared/Projects/01_PhD/04_DeepGR4J_QNN/deepgr4j-extremes/results/gr4j/result.csv'\n",
    "hybrid_ds = HybridDataset(data_dir=camels_dir, gr4j_logfile=gr4j_logfile, \n",
    "                        #   station_list=['314213'],\n",
    "                          state_outlet='WA', map_zone=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds, test_ds = hybrid_ds.get_datasets(batch_size=256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((166114, 7, 8), (166114, 7), (166114, 1))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hybrid_ds.ts_arr.shape, hybrid_ds.static_arr.shape, hybrid_ds.target_arr.shape  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(256,) (256, 7, 8) (256, 7) (256, 1)\n"
     ]
    }
   ],
   "source": [
    "for batch in train_ds:\n",
    "    print(batch[0].shape, batch[1].shape, batch[2].shape, batch[3].shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hydroml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
